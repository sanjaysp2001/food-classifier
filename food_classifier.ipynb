{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "food-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjaysp2001/food-classifier/blob/master/food_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFNHGzzD4An9"
      },
      "source": [
        "Download and Extract the Food-101 Dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqPLzUORIgPl"
      },
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz \n",
        "!tar xzvf food-101.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwFOuWO26Wma"
      },
      "source": [
        "Import the necessary modules. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4yn2F72dbCF"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import stat\n",
        "import seaborn as sns\n",
        "import collections\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.image as img\n",
        "import random\n",
        "import cv2\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from collections import defaultdict\n",
        "from ipywidgets import interact, interactive, fixed\n",
        "import ipywidgets as widgets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.io import imread\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import load_model\n",
        "from shutil import copy\n",
        "from shutil import copytree, rmtree\n",
        "import tensorflow.keras.backend \n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivj-jX8IwJAX"
      },
      "source": [
        "Generating essential dictionaries and methods that will be make life simple.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhh1o5s2Goew"
      },
      "source": [
        "class_N = {}\n",
        "N_class = {}\n",
        "with open('food-101/meta/classes.txt', 'r') as txt:\n",
        "    classes = [i.strip() for i in txt.readlines()]\n",
        "    class_N = dict(zip(classes, range(len(classes))))\n",
        "    N_class = dict(zip(range(len(classes)), classes))\n",
        "    class_N = {i: j for j, i in N_class.items()}\n",
        "class_N_sorted = collections.OrderedDict(sorted(class_N.items()))\n",
        "print(class_N)\n",
        "\n",
        "# Method to generate directory-file map. \n",
        "def gen_dir_file_map(path):\n",
        "    dir_files = defaultdict(list)\n",
        "    with open(path, 'r') as txt:\n",
        "        files = [i.strip() for i in txt.readlines()]\n",
        "        for f in files:\n",
        "            dir_name, id = f.split('/')\n",
        "            dir_files[dir_name].append(id + '.jpg')\n",
        "    return dir_files\n",
        "\n",
        "# Method to recursively copy a directory.  \n",
        "def copytree(source, target, symlinks = False, ignore = None):\n",
        "  if not os.path.exists(target):\n",
        "      os.makedirs(target)\n",
        "      shutil.copystat(source, target)\n",
        "  data = os.listdir(source)\n",
        "  if ignore:\n",
        "      exclude = ignore(source, data)\n",
        "      data = [x for x in data if x not in exclude]\n",
        "  for item in data:\n",
        "      src = os.path.join(source, item)\n",
        "      dest = os.path.join(target, item)\n",
        "      if symlinks and os.path.islink(src):\n",
        "          if os.path.lexists(dest):\n",
        "              os.remove(dest)\n",
        "          os.symlink(os.readlink(src), dest)\n",
        "          try:\n",
        "              st = os.lstat(src)\n",
        "              mode = stat.S_IMODE(st.st_mode)\n",
        "              os.lchmod(dest, mode)\n",
        "          except:\n",
        "              pass\n",
        "      elif os.path.isdir(src):\n",
        "          copytree(src, dest, symlinks, ignore)\n",
        "      else:\n",
        "          shutil.copy2(src, dest)\n",
        "\n",
        "# Train files to ignore. \n",
        "def ignore_train(d, filenames):\n",
        "  subdir = d.split('/')[-1]\n",
        "  train_dir_files = gen_dir_file_map('food-101/meta/train.txt')\n",
        "  to_ignore = train_dir_files[subdir]\n",
        "  return to_ignore\n",
        "\n",
        "# Test files to ignore.    \n",
        "def ignore_test(d, filenames):\n",
        "  subdir = d.split('/')[-1]\n",
        "  test_dir_files = gen_dir_file_map('food-101/meta/test.txt')\n",
        "  to_ignore = test_dir_files[subdir]\n",
        "  return to_ignore\n",
        "\n",
        "# Method to load and resize images.  \n",
        "def load_images(path_to_imgs):\n",
        "  resize_count = 0\n",
        "  \n",
        "  invalid_count = 0\n",
        "  all_imgs = []\n",
        "  all_classes = []\n",
        "\n",
        "  for i, subdir in enumerate(listdir(path_to_imgs)):\n",
        "      imgs = listdir(join(path_to_imgs, subdir))\n",
        "      classN = class_N[subdir]\n",
        "      for img_name in imgs:\n",
        "          img_arr = cv2.imread(join(path_to_imgs, subdir, img_name))\n",
        "          img_arr_rs = img_arr\n",
        "          img_arr_rs = cv2.resize(img_arr, (200,200),interpolation=cv2.INTER_AREA)\n",
        "          resize_count += 1\n",
        "          im_rgb = cv2.cvtColor(img_arr_rs, cv2.COLOR_BGR2RGB)\n",
        "          all_imgs.append(im_rgb)\n",
        "          all_classes.append(classN)\n",
        "  \n",
        "  return np.array(all_imgs), np.array(all_classes)\n",
        "\n",
        "# Method to generate train-test files. \n",
        "def gen_train_test_split(path_to_imgs = 'food-101/images' , target_path = 'food-101'):\n",
        "  copytree(path_to_imgs, target_path + '/train', ignore=ignore_test)\n",
        "  copytree(path_to_imgs, target_path + '/test', ignore=ignore_train)\n",
        "\n",
        "# Method to load train-test files.\n",
        "def load_train_test_data(path_to_train_imgs, path_to_test_imgs):\n",
        "  X_train, y_train = load_images(path_to_train_imgs)\n",
        "  X_test, y_test = load_images(path_to_test_imgs)\n",
        "  return X_train, y_train, X_test, y_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4pZxP7GfTqD"
      },
      "source": [
        "# Generate train-test files. \n",
        "if not os.path.isdir('./food-101/test') and not os.path.isdir('./food-101/train'):\n",
        "    gen_train_test_split()  \n",
        "    len_train = len(os.listdir('/content/food-101/train'))\n",
        "    len_test = len(os.listdir('/content/food-101/test'))\n",
        "    print(len_train,len_test)\n",
        "else:\n",
        "    print('train and test folders already exists.')\n",
        "    len_train = len(os.listdir('/content/food-101/train'))\n",
        "    len_test = len(os.listdir('/content/food-101/test'))\n",
        "    print(len_train,len_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-XRjjAiaDrv"
      },
      "source": [
        "# List of all the food classes.\n",
        "foods_sorted = sorted(os.listdir('food-101/images'))\n",
        "foods_sorted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bQ0zNhu0Cw-"
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_train_test_data('/content/food-101/train','/content/food-101/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXwcn32r4Tui"
      },
      "source": [
        "Fitting the data on the Inception-v3 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uCVS0w4d3Jy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "a6169983-c8a8-479b-f7b0-0440408fe279"
      },
      "source": [
        "tensorflow.keras.backend.clear_session()\n",
        "\n",
        "n_classes = 101\n",
        "batch_size = 16\n",
        "width, height = 200, 200\n",
        "train_data = '/content/food-101/train'\n",
        "test_data = '/content/food-101/test'\n",
        "train_samples = 75750\n",
        "test_samples = 25250\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1. / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_data_gen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_gen = train_data_gen.flow_from_directory(train_data, target_size=(height, width), batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "test_gen = test_data_gen.flow_from_directory(test_data, target_size=(height, width), batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "inception = InceptionV3(weights='imagenet', include_top=False)\n",
        "layer = inception.output\n",
        "layer = GlobalAveragePooling2D()(layer)\n",
        "layer = Dense(128,activation='relu')(layer)\n",
        "layer = Dropout(0.2)(layer)\n",
        "\n",
        "predictions = Dense(n_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(layer)\n",
        "\n",
        "model = Model(inputs=inception.input, outputs=predictions)\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath='best_model_101class.hdf5', save_best_only=True)\n",
        "csv_logger = CSVLogger('history_101class.log')\n",
        "\n",
        "history_101class = model.fit(train_gen, steps_per_epoch= train_samples // batch_size, validation_data= test_gen, validation_steps= test_samples // batch_size, epochs=30, callbacks=[csv_logger, checkpointer])\n",
        "\n",
        "model.save('model_trained_101class.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 75750 images belonging to 101 classes.\n",
            "Found 25250 images belonging to 101 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "Epoch 1/30\n",
            "4734/4734 [==============================] - 1067s 225ms/step - loss: 4.9407 - accuracy: 0.0616 - val_loss: 4.2642 - val_accuracy: 0.1995\n",
            "Epoch 2/30\n",
            "4734/4734 [==============================] - 940s 198ms/step - loss: 3.9720 - accuracy: 0.2322 - val_loss: 2.9427 - val_accuracy: 0.4301\n",
            "Epoch 3/30\n",
            "4734/4734 [==============================] - 937s 198ms/step - loss: 3.1980 - accuracy: 0.3630 - val_loss: 2.3596 - val_accuracy: 0.5322\n",
            "Epoch 4/30\n",
            "4734/4734 [==============================] - 944s 199ms/step - loss: 2.7665 - accuracy: 0.4433 - val_loss: 2.0358 - val_accuracy: 0.5941\n",
            "Epoch 5/30\n",
            "4734/4734 [==============================] - 961s 203ms/step - loss: 2.4826 - accuracy: 0.4981 - val_loss: 1.8426 - val_accuracy: 0.6304\n",
            "Epoch 6/30\n",
            "4734/4734 [==============================] - 946s 200ms/step - loss: 2.2733 - accuracy: 0.5392 - val_loss: 1.6932 - val_accuracy: 0.6582\n",
            "Epoch 7/30\n",
            "4734/4734 [==============================] - 954s 201ms/step - loss: 2.1126 - accuracy: 0.5687 - val_loss: 1.5918 - val_accuracy: 0.6742\n",
            "Epoch 8/30\n",
            "4734/4734 [==============================] - 940s 199ms/step - loss: 1.9707 - accuracy: 0.5968 - val_loss: 1.4980 - val_accuracy: 0.6952\n",
            "Epoch 9/30\n",
            "4734/4734 [==============================] - 919s 194ms/step - loss: 1.8638 - accuracy: 0.6183 - val_loss: 1.4207 - val_accuracy: 0.7104\n",
            "Epoch 10/30\n",
            "4734/4734 [==============================] - 921s 195ms/step - loss: 1.7649 - accuracy: 0.6360 - val_loss: 1.3897 - val_accuracy: 0.7164\n",
            "Epoch 11/30\n",
            "4734/4734 [==============================] - 922s 195ms/step - loss: 1.6807 - accuracy: 0.6542 - val_loss: 1.3131 - val_accuracy: 0.7311\n",
            "Epoch 12/30\n",
            "4734/4734 [==============================] - 925s 195ms/step - loss: 1.5900 - accuracy: 0.6719 - val_loss: 1.2904 - val_accuracy: 0.7312\n",
            "Epoch 13/30\n",
            "1047/4734 [=====>........................] - ETA: 10:41 - loss: 1.5542 - accuracy: 0.6802Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXuXAyubeA5G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "830597c6-e4cd-4bf9-c472-eaeac8dd5ebd"
      },
      "source": [
        "class_map_101 = train_gen.class_indices\n",
        "class_map_101"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apple_pie': 0,\n",
              " 'baby_back_ribs': 1,\n",
              " 'baklava': 2,\n",
              " 'beef_carpaccio': 3,\n",
              " 'beef_tartare': 4,\n",
              " 'beet_salad': 5,\n",
              " 'beignets': 6,\n",
              " 'bibimbap': 7,\n",
              " 'bread_pudding': 8,\n",
              " 'breakfast_burrito': 9,\n",
              " 'bruschetta': 10,\n",
              " 'caesar_salad': 11,\n",
              " 'cannoli': 12,\n",
              " 'caprese_salad': 13,\n",
              " 'carrot_cake': 14,\n",
              " 'ceviche': 15,\n",
              " 'cheese_plate': 16,\n",
              " 'cheesecake': 17,\n",
              " 'chicken_curry': 18,\n",
              " 'chicken_quesadilla': 19,\n",
              " 'chicken_wings': 20,\n",
              " 'chocolate_cake': 21,\n",
              " 'chocolate_mousse': 22,\n",
              " 'churros': 23,\n",
              " 'clam_chowder': 24,\n",
              " 'club_sandwich': 25,\n",
              " 'crab_cakes': 26,\n",
              " 'creme_brulee': 27,\n",
              " 'croque_madame': 28,\n",
              " 'cup_cakes': 29,\n",
              " 'deviled_eggs': 30,\n",
              " 'donuts': 31,\n",
              " 'dumplings': 32,\n",
              " 'edamame': 33,\n",
              " 'eggs_benedict': 34,\n",
              " 'escargots': 35,\n",
              " 'falafel': 36,\n",
              " 'filet_mignon': 37,\n",
              " 'fish_and_chips': 38,\n",
              " 'foie_gras': 39,\n",
              " 'french_fries': 40,\n",
              " 'french_onion_soup': 41,\n",
              " 'french_toast': 42,\n",
              " 'fried_calamari': 43,\n",
              " 'fried_rice': 44,\n",
              " 'frozen_yogurt': 45,\n",
              " 'garlic_bread': 46,\n",
              " 'gnocchi': 47,\n",
              " 'greek_salad': 48,\n",
              " 'grilled_cheese_sandwich': 49,\n",
              " 'grilled_salmon': 50,\n",
              " 'guacamole': 51,\n",
              " 'gyoza': 52,\n",
              " 'hamburger': 53,\n",
              " 'hot_and_sour_soup': 54,\n",
              " 'hot_dog': 55,\n",
              " 'huevos_rancheros': 56,\n",
              " 'hummus': 57,\n",
              " 'ice_cream': 58,\n",
              " 'lasagna': 59,\n",
              " 'lobster_bisque': 60,\n",
              " 'lobster_roll_sandwich': 61,\n",
              " 'macaroni_and_cheese': 62,\n",
              " 'macarons': 63,\n",
              " 'miso_soup': 64,\n",
              " 'mussels': 65,\n",
              " 'nachos': 66,\n",
              " 'omelette': 67,\n",
              " 'onion_rings': 68,\n",
              " 'oysters': 69,\n",
              " 'pad_thai': 70,\n",
              " 'paella': 71,\n",
              " 'pancakes': 72,\n",
              " 'panna_cotta': 73,\n",
              " 'peking_duck': 74,\n",
              " 'pho': 75,\n",
              " 'pizza': 76,\n",
              " 'pork_chop': 77,\n",
              " 'poutine': 78,\n",
              " 'prime_rib': 79,\n",
              " 'pulled_pork_sandwich': 80,\n",
              " 'ramen': 81,\n",
              " 'ravioli': 82,\n",
              " 'red_velvet_cake': 83,\n",
              " 'risotto': 84,\n",
              " 'samosa': 85,\n",
              " 'sashimi': 86,\n",
              " 'scallops': 87,\n",
              " 'seaweed_salad': 88,\n",
              " 'shrimp_and_grits': 89,\n",
              " 'spaghetti_bolognese': 90,\n",
              " 'spaghetti_carbonara': 91,\n",
              " 'spring_rolls': 92,\n",
              " 'steak': 93,\n",
              " 'strawberry_shortcake': 94,\n",
              " 'sushi': 95,\n",
              " 'tacos': 96,\n",
              " 'takoyaki': 97,\n",
              " 'tiramisu': 98,\n",
              " 'tuna_tartare': 99,\n",
              " 'waffles': 100}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHvBPtIzOcEX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}